{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWJ98-vpbcSr",
        "outputId": "38b7375e-f3ab-43c2-f325-8b23fc0a63ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights: [ 0.30115786  0.12276031 -0.11469102 -0.11556772 -0.13229105  0.41705124]\n",
            "Modified Weights: [ 0.30189883  0.12218828 -0.11557031 -0.11498384 -0.13195805  0.2143759\n",
            "  0.2143759 ]\n"
          ]
        }
      ],
      "source": [
        "# @title Question 1\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Generating synthetic data\n",
        "np.random.seed(0)\n",
        "X_data = np.random.randn(100, 6)  # 100 samples, 6 features\n",
        "y_data = np.random.randint(0, 2, 100)  # Binary target 0/1\n",
        "\n",
        "# Training an initial logistic regression model on the original data\n",
        "original_model = LogisticRegression()\n",
        "original_model.fit(X_data, y_data)\n",
        "initial_weights = original_model.coef_[0]\n",
        "\n",
        "# Duplicating the last feature in the dataset\n",
        "X_modified = np.column_stack((X_data, X_data[:, -1]))\n",
        "\n",
        "# Training a new logistic regression model on the modified data\n",
        "modified_model = LogisticRegression()\n",
        "modified_model.fit(X_modified, y_data)\n",
        "modified_weights = modified_model.coef_[0]\n",
        "\n",
        "print(\"Initial Weights:\", initial_weights)\n",
        "print(\"Modified Weights:\", modified_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created a logistic regression model\n",
        "\n",
        "Observations:\n",
        "\n",
        "The weights from w_0 to w_n-1 for original and new model are approximately equal\n",
        "For w_n (original) & (w_n_new, w_n+1_new): approx relation is w_n = w_n_new + w_n+1_new"
      ],
      "metadata": {
        "id": "BMHDJ_fEbwr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Question 2\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Click-through data (as given in %) [clicks, no-clicks] and 1000 templates of each are sent\n",
        "clicks_A = 100\n",
        "no_clicks_A = 900\n",
        "\n",
        "clicks_B = 70\n",
        "no_clicks_B = 930\n",
        "\n",
        "clicks_C = 85\n",
        "no_clicks_C = 915\n",
        "\n",
        "clicks_D = 120\n",
        "no_clicks_D = 880\n",
        "\n",
        "clicks_E = 140\n",
        "no_clicks_E = 860\n",
        "\n",
        "# Function to perform chi-squared test and return p-value\n",
        "def calculate_p_value(control_clicks, control_no_clicks, other_clicks, other_no_clicks):\n",
        "    data = [[control_clicks, control_no_clicks], [other_clicks, other_no_clicks]]\n",
        "    _, p_value, _, _ = chi2_contingency(data)\n",
        "    return p_value\n",
        "\n",
        "# Calculate p-values for each template compared to A\n",
        "p_values_dict = {\n",
        "    \"B vs A\": calculate_p_value(clicks_A, no_clicks_A, clicks_B, no_clicks_B),\n",
        "    \"C vs A\": calculate_p_value(clicks_A, no_clicks_A, clicks_C, no_clicks_C),\n",
        "    \"D vs A\": calculate_p_value(clicks_A, no_clicks_A, clicks_D, no_clicks_D),\n",
        "    \"E vs A\": calculate_p_value(clicks_A, no_clicks_A, clicks_E, no_clicks_E)\n",
        "}\n",
        "\n",
        "p_values_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdosODuib-yh",
        "outputId": "5b068846-9e84-4638-bd9c-270f55992d07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B vs A': 0.020060502655718262,\n",
              " 'C vs A': 0.2799261382501793,\n",
              " 'D vs A': 0.17451579008209805,\n",
              " 'E vs A': 0.007283436889671482}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template A gets 10% click through rate (CTR), B gets 7% CTR, C gets 8.5% CTR, D gets 12% CTR and E gets 14% CTR.\n",
        "\n",
        "Using: Chi-squared test for proportions\n",
        "\n",
        "Null hypothesis H0 for each comparison is that there is no difference between the CTRs of the control template (A) and the other templates (B, C, D, E).\n",
        "\n",
        "Alternative hypothesis Ha is that there is a difference.\n",
        "\n",
        "We are looking for a 95% confidence level to reject the null hypothesis. (p-value: 0.05)"
      ],
      "metadata": {
        "id": "-yKRfyeDcPs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "p-value for BvsA - 0.02 < 0.05 Reject H0 => there is a difference (B is worse than A)\n",
        "\n",
        "p-value for CvsA - 0.28 > 0.05 Fail to reject H0 => there is no difference between A & C\n",
        "\n",
        "p-value for DvsA - 0.17 > 0.05 Fail to reject H0 => there is no difference between A & D\n",
        "\n",
        "p-value for EvsA - 0.007 < 0.05 Reject H0 => there is a difference (E is better than A)\n",
        "\n",
        "b. E is better than A with over 95% confidence, B is worse than A with over 95% confidence. You need to run the test for longer to tell where C and D compare to A with 95% confidence."
      ],
      "metadata": {
        "id": "b_zc5q47cS10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Question 3\n"
      ],
      "metadata": {
        "id": "MYvfrJf6c2E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximate computational cost of each gradient descent iteration of logistic regression:\n",
        "\n",
        "The gradient of cost function wrt to weights is: ∇J(θ)= (1/m)(h(Xθ)−y)X\n",
        "\n",
        "Computational Cost: Matrix Multiplication ( Xθ ): Since each row of  X  has, on average,  k  non-zero entries, and there are  m  such rows, the total number of non-zero entries that need to be considered in the multiplication is about  mk . For sparse matrix-vector multiplication, the cost is proportional to the number of non-zero elements, so the cost is  O(mk) .\n",
        "\n",
        "Computing the Hypothesis ( h(Xθ) ): This step involves applying the sigmoid function to each of the  m  results of the matrix-vector multiplication. The cost is  O(m) .\n",
        "\n",
        "Gradient Calculation ( XT(h(Xθ)−y) ): Again, the key part of this computation is the matrix-vector multiplication involving the sparse matrix  XT . The number of non-zero elements remains the same as in the first step, so the cost is also  O(mk) .\n",
        "\n",
        "Overall Computational Cost:  O(mk)"
      ],
      "metadata": {
        "id": "7XSROf3Dc2ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Question 4\n"
      ],
      "metadata": {
        "id": "yM_rnjZ7dMT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Ranking in Terms of Accuracy:\n",
        "\n",
        "Method 2 (Random Labeled Stories): By training on a diverse and representative sample, V2 is likely to develop a more general understanding of the categories across different news sources. This method is expected to yield the highest accuracy in classifying a broad range of articles.\n",
        "\n",
        "Method 3 (Wrong and Farthest from the Decision Boundary): Learning from the most confident mistakes of V1 could address specific weaknesses but this will lack generality and can lead to overfitting, might make this approach less effective.\n",
        "\n",
        "Method 1 (Closest to the Decision Boundary): Although valuable for fine-tuning the decision boundary, the focus on edge cases might not contribute as significantly to overall accuracy improvement compared to the other method"
      ],
      "metadata": {
        "id": "ZvuneTfZc6fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Question 5\n"
      ],
      "metadata": {
        "id": "40oAqPuQdCRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum Likelihood Estimate (MLE):\n",
        "\n",
        "The MLE for a binomial distribution (which is appropriate for coin tosses) is simply the ratio of the number of successes (heads) to the total number of trials. Therefore, the MLE of p is k/n\n",
        "\n",
        "Bayesian Estimate with Uniform Prior: For the Bayesian estimate, we start with a uniform prior for p over [0, 1]. This is equivalent to a Beta distribution with parameters α = 1, β = 1\n",
        "\n",
        "Now for k heads in n tosses, the posterior distribution for p is a Beta distribution with α = k+1, β = n-k+1\n",
        "\n",
        "Bayesian Estimate of p is  (k+1)/(n+2)\n",
        "\n",
        "Maximum a Posteriori (MAP) Estimate with Uniform Prior: k/n (assuming k>0 and n-k>0)"
      ],
      "metadata": {
        "id": "5moBvj8Hc9zu"
      }
    }
  ]
}